---
title: "ec340_script3"
output: html_document
---

```{r setup, include=FALSE}
# Run once for installation
# Quandl API key: rTCfeqfv2pbfQG-uk-af
# install.packages("Quandl")
# install.packages("keras")
# install_keras()
# install.packages("tensorflow")
# install.packages("tsibble")
# install.packages("randomforest")
# install.packages("forecast")
# install.packages("aTSA)
# install.packages("xgboost")
# install.packages("superml")
# install.packages("rfUtilities")
# install.packages("gridExtra")
# install.packages("ggplotify")

knitr::opts_chunk$set(echo = TRUE)
library(tidyverse) # For loading the generally needed packages, dplyr, ggplot2 etc
library(tsibble) # Extension of the tidyverse to time-series data
library(Quandl) # API for downloading the data
library(keras) # For Artificial Neural Network
library(randomForest) # To implement random forests
library(forecast) # To assess predictions and compare them to the real test set
library(aTSA) #Useful for time series analysis
library(xgboost) # For gradient boosted random forests
#library(e1071) # For 
library(superml) # For cross validation of the XGB model
library(lmtest) # For testing the basic, linear model
library(rfUtilities) #For cross validation of the random forest
library(gridExtra) 
library(ggplotify)
set.seed(177125)
```

```{r download-data}
# We begin by downloading the data from Quandl via their in-R API.
# The Hong Kong stock exchange is used, and the HSBC stock price was randomly selected.
# For a full list of potential stocks, please see: https://www.quandl.com/data/HKEX-Hong-Kong-Exchange
# We generate a timeseries object as well as a tsibble object as both will be useful for different parts of analysis.
Quandl.api_key('rTCfeqfv2pbfQG-uk-af')
hsbc_tib <- Quandl('HKEX/00005', column_index = "1") %>% 
  as_tsibble(index="Date")

names(hsbc_tib)[2] <- "Price"
dates <- seq(as.Date("2014-02-21"), as.Date("2021-05-07"), by = "day")
hsbc_ts <- as.ts(hsbc_tib, start = c(2014, as.numeric(format(dates[1], "%j"))), frequency=365)
```

```{r inspect-for-missing, echo=TRUE, results="hide"}
# We check for missing/NA values in the data.
has_gaps(hsbc_tib)
colSums(is.na(hsbc_tib[, "Price"]))
```

```{r impute-missing-values}
# Create a function to fill missing values in the time series (due to weekends/bank holidays etc).
# Each NA value is replaced with the previous close price of the series. This is the price that market
# participants would see when making trading decisions on days where the exchange was closed.
ImputeMissing <- function(ts){
  df <- data.frame(ts)
  for(i in 1:nrow(df)){
    if(is.na(df[i, 1])){
      df[i, 1] <- df[i-1, 1]
    }
  }
  dates_formatted <- data.frame(dates)
  df <- cbind(df, dates_formatted)
  names(df)[1] <- "Price"
  names(df)[2] <- "Date"
  tib_new <- as_tsibble(df, index="Date") 

  return(tib_new)
}

hsbc_tib <- ImputeMissing(hsbc_ts)
hsbc_ts <- as.ts(hsbc_tib, start = c(2014, as.numeric(format(dates[1], "%j"))), frequency=365)
```

```{r visualise-data}
# To first get a sense of the data, we plot the priceseries, autocorrelation and partial
# autocorrelation functions.
price_plot <- hsbc_tib %>% 
  ggplot(aes(Date, Price)) +
  geom_line() +
  theme_minimal() +
  labs(title = "HSBC Stock Price", x = "Year", y = "Price")

acf_plot <- hsbc_tib %>%
  ggAcf(lag.max=30,
        type="correlation",
        plot=TRUE)

pacf_plot <- hsbc_tib %>%
  ggPacf(lag.max=30,
         plot=TRUE)

print(price_plot)
print(acf_plot)
print(pacf_plot)
```

```{r stationarity, results="hide"}
# Augmented Dickey-Fuller test to assess whether price-series is stationary.
# Transform series by taking the natural log, and then the first difference (assuming
# that the series is I(1)). Another ADF test reveals this transformed series is stationary.
adf.test(hsbc_tib$Price)

hsbc_log_diff <- hsbc_ts %>%
  log() %>%
  diff(1)

adf.test(hsbc_log_diff)

#Define the function to reverse the transformation
ReverseLogDiff <- function(data){
  data <- data %>%
  cumsum() %>%
  exp()
  data <- data * as.vector(tail(hsbc_ts, 1))
  return(data)
}
```

```{r train-val-test-split-and-normalisation}
# Using the inbuilt embedding function, lags of the series are generated to be used as features.
# Both the stationary, and non-stationary series are embedded as both will be used for analysis.

hsbc_normalised <- scale(log(hsbc_ts))

ReverseNorm <- function(data){
  data <- (data*attr(hsbc_normalised, "scaled:scale") + attr(hsbc_normalised, "scaled:center")) %>%
    exp()
  return(data)
}

hsbc_lagged <- embed(hsbc_normalised, 21)
hsbc_log_diff_lagged <- embed(hsbc_log_diff, 21)

split_num_val <- round(nrow(hsbc_lagged)*0.7) # 10% of data reserved for validation
split_num_train <- round(nrow(hsbc_lagged)*0.6) #30% of data reserved for testing

hsbc_train <- data.frame(hsbc_lagged[1: split_num_train, ])
hsbc_val <- data.frame(hsbc_lagged[(split_num_train+1):split_num_val, ])
hsbc_test <- data.frame(hsbc_lagged[(split_num_val+1):nrow(hsbc_lagged), ])
hsbc_log_diff_train <- data.frame(hsbc_log_diff_lagged[1: split_num_train, ])
hsbc_log_diff_val <- data.frame(hsbc_log_diff_lagged[(split_num_train+1):split_num_val, ])
hsbc_log_diff_test <- data.frame(hsbc_log_diff_lagged[(split_num_val+1):nrow(hsbc_log_diff_lagged), ])

X_train <- data.matrix(hsbc_train[, -1])
X_train_lstm <- array(X_train, c(dim(X_train), 1))
y_train <- hsbc_train[, 1]
X_log_diff_train <- data.matrix(hsbc_log_diff_train[, -1])
X_log_diff_train_lstm <- array(X_log_diff_train, c(dim(X_log_diff_train), 1))
y_log_diff_train <- hsbc_log_diff_train[, 1]

X_val <- data.matrix(hsbc_val[, -1])
X_val_lstm <- array(X_val, c(dim(X_val), 1))
y_val <- hsbc_val[, 1]
val_list <- list(X_val_lstm, y_val)
X_log_diff_val <- data.matrix(hsbc_log_diff_val[, -1])
X_log_diff_val_lstm <- array(X_log_diff_val, c(dim(X_log_diff_val), 1))
y_log_diff_val <- hsbc_log_diff_val[, 1]
val_log_diff_list <- list(X_log_diff_val_lstm, y_log_diff_val)

X_test <- data.matrix(hsbc_test[, -1])
X_test_lstm <- array(X_test, c(dim(X_test), 1))
y_test <- hsbc_test[, 1] %>%
  ReverseNorm()
X_log_diff_test <- data.matrix(hsbc_log_diff_test[, -1])
X_log_diff_test_lstm <- array(X_log_diff_test, c(dim(X_log_diff_test), 1))
y_log_diff_test <- hsbc_log_diff_test[, 1] %>%
  ReverseLogDiff()
```

```{r linear-model}
linear_norm <- lm(X1 ~ X2 + X3 + X4 + X5 + X6 + X7 + X8 + X9 + X10 + X11 + X12 + X13 + X14 + X15 + X16 + X17 + X18 + X19 + X20 + X21, data=hsbc_train)
summary(linear_norm)
resettest(linear_norm)

linear_predicted <- predict.lm(linear, newdata = data.frame(hsbc_test)) %>% 
  ReverseNorm()

linear_accuracy <- forecast::accuracy(linear_predicted, y_test)
linear_accuracy
```

```{r random-forest-norm}
rf <- randomForest(X_train, y_train,
                   xtest=X_val, ytest=y_val,
                   keep.forest = TRUE)

rf_cv <- rf.crossValidation(rf, X_train, y_train,
                            bootstrap=FALSE, xtest=data.frame(X_val), ytest=y_val,
                            keep.forest = TRUE)

rf_cv_predicted <- predict(rf, X_test) %>%
  ReverseNorm()

rf_accuracy <- forecast::accuracy(rf_cv_predicted, y_test)
rf_accuracy
```

```{r random-forest-log-diff}
rf_log_diff <- randomForest(X_log_diff_train, y_log_diff_train,
                   xtest=X_log_diff_val, ytest=y_log_diff_val,
                   keep.forest = TRUE)

rf_cv_log_diff <- rf.crossValidation(rf, X_log_diff_train, y_log_diff_train,
                            bootstrap=FALSE, xtest=data.frame(X_log_diff_val), ytest=y_log_diff_val,
                            keep.forest = TRUE)

rf_cv_log_diff_predicted <- predict(rf, X_log_diff_test) %>%
  ReverseNorm()

rf_log_diff_accuracy <- forecast::accuracy(rf_cv_log_diff_predicted, y_log_diff_test)
rf_log_diff_accuracy
```

```{r xgb-norm}
xgb_norm <- XGBTrainer$new(objective="reg:squarederror")

xgb_gs_norm <- GridSearchCV$new(trainer=xgb_norm,
                           parameters=list(learning_rate=seq(from=0.1, to=0.5, by=0.05),
                                           max_depth=seq(from=3, to=10, by=1),
                                           subsample=seq(from=0.4, to=1, by=0.1),
                                           n_estimators=seq(from=50, to=300, by=50)),
                           n_folds=5,
                           scoring="rmse")

xgb_gs_norm$fit(hsbc_train, "X1")

xgb_gs_norm$best_iteration()

xgb_norm_tuned <- xgboost(data=X_train,
                     label=y_train,
                     nrounds=200,
                     params=list(eta=0.5,
                                 max_depth=8,
                                 subsample=0.4,
                                 n_estimators=250))

xgb_norm_predicted <- predict(xgb_norm_tuned, X_test) %>%
  ReverseNorm()

xgb_accuracy <- forecast::accuracy(xgb_tuned_predicted, y_test)
xgb_accuracy
```

```{r xgb-norm}
xgb_log_diff <- XGBTrainer$new(objective="reg:squarederror")

xgb_gs_log_diff <- GridSearchCV$new(trainer=xgb_log_diff,
                           parameters=list(learning_rate=seq(from=0.1, to=0.5, by=0.05),
                                           max_depth=seq(from=3, to=10, by=1),
                                           subsample=seq(from=0.4, to=1, by=0.1),
                                           n_estimators=seq(from=50, to=300, by=50)),
                           n_folds=5,
                           scoring="rmse")

xgb_gs_log_diff$fit(hsbc_log_diff_train, "X1")

xgb_gs_log_diff$best_iteration()

xgb_log_diff_tuned <- xgboost(data=X_log_diff_train,
                     label=y_log_diff_train,
                     nrounds=200,
                     params=list(eta=0.5,
                                 max_depth=8,
                                 subsample=0.4,
                                 n_estimators=250))

xgb_log_diff_predicted <- predict(xgb_log_diff_tuned, X_log_diff_test) %>%
  ReverseNorm()

xgb_log_diff_accuracy <- forecast::accuracy(xgb_log_diff_predicted, y_log_diff_test)
xgb_log_diff_accuracy
```



```{r results}
results <- data.frame(cbind(y_test, linear_predicted))
names(results)[1] <- "actual"
results <- data.frame(cbind(results, rf_cv_predicted))
results <- data.frame(cbind(results, rf_cv_log_diff_predicted))
results <- data.frame(cbind(results, xgb_norm_predicted))
results <- data.frame(cbind(results, xgb_log_diff_predicted))
```
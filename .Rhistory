diff(1)
split_num_val <- round(nrow(hsbc_lagged)*0.7)
hsbc_lagged <- embed(hsbc_log_diff, 21)
split_num_val <- round(nrow(hsbc_lagged)*0.7)
split_num_train <- round(nrow(hsbc_lagged)*0.6)
hsbc_train <- data.frame(hsbc_lagged[1: split_num_train, ])
hsbc_val <- data.frame(hsbc_lagged[(split_num_train+1):split_num_val, ])
hsbc_test <- data.frame(hsbc_lagged[(split_num_val+1):nrow(hsbc_lagged), ])
X_train <- data.matrix(hsbc_train[, -1])
X_train_lstm <- array(X_train, c(dim(X_train), 1))
y_train <- hsbc_train[, 1]
X_val <- data.matrix(hsbc_val[, -1])
X_val_lstm <- array(X_val, c(dim(X_val), 1))
y_val <- hsbc_val[, 1]
X_test <- data.matrix(hsbc_test[, -1])
X_test_lstm <- array(X_test, c(dim(X_test), 1))
y_test <- hsbc_test[, 1] %>%
cumsum() %>%
exp()
y_test <- y_test * as.vector(tail(hsbc_ts, 1))
val_list <- list(X_val_lstm, y_val)
?LMTrainer
linear <- lm(X1 ~ X2 + X3 + X4 + X5 + X6 + X7 + X8 + X9 + X10 + X11 + X12 + X13 + X14 + X15 + X16 + X17 + X18 + X19 + X20 + X21, data=hsbc_train)
summary(linear)
linear_predicted <- predict.lm(linear, newdata = data.frame(hsbc_test)) %>%
cumsum() %>%
exp()
linear_predicted <- linear_predicted * as.vector(tail(hsbc_ts, 1))
results <- data.frame(cbind(y_test, linear_predicted))
names(results)[1] <- "actual"
linear_accuracy <- accuracy(linear_predicted, y_test)
# Run once for installation
# Quandl API key: rTCfeqfv2pbfQG-uk-af
# install.packages("Quandl")
# install.packages("keras")
# install_keras()
# install.packages("tensorflow")
# install.packages("tsibble")
# install.packages("randomforest")
# install.packages("forecast")
# install.packages("aTSA)
# install.packages("xgboost")
# install.packages("superml")
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse) # For loading the generally needed packages, dplyr, ggplot2 etc
library(tsibble) # Extension of the tidyverse to time-series data
library(Quandl) # API for downloading the data
library(keras) # To train the LSTM model
library(randomForest) # To implement random forests
library(forecast) # To assess predictions and compare them to the real test set
library(aTSA)
library(xgboost)
library(e1071)
library(superml)
library(lmtest)
resettest(linear)
?RFTrainer
rf <- RFTrainer$new(max_features=none, classification=0)
rf_fit <- rf$fit(X=hsbc_train, y="X1")
rf <- RFTrainer$new(max_features=none, classification=0)
rf$fit(X=hsbc_train, y="X1")
# Run once for installation
# Quandl API key: rTCfeqfv2pbfQG-uk-af
# install.packages("Quandl")
# install.packages("keras")
# install_keras()
# install.packages("tensorflow")
# install.packages("tsibble")
# install.packages("randomforest")
# install.packages("forecast")
# install.packages("aTSA)
# install.packages("xgboost")
# install.packages("superml")
# install.packages("rfUtilities")
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse) # For loading the generally needed packages, dplyr, ggplot2 etc
library(tsibble) # Extension of the tidyverse to time-series data
library(Quandl) # API for downloading the data
library(keras) # To train the LSTM model
library(randomForest) # To implement random forests
library(forecast) # To assess predictions and compare them to the real test set
library(aTSA)
library(xgboost)
library(e1071)
library(superml)
library(lmtest)
library(rfUtilities)
linear <- lm(X1 ~ X2 + X3 + X4 + X5 + X6 + X7 + X8 + X9 + X10 + X11 + X12 + X13 + X14 + X15 + X16 + X17 + X18 + X19 + X20 + X21, data=hsbc_train)
summary(linear)
resettest(linear)
rf <- randomForest(X_train, y_train, xtest=X_val, ytest=y_val, keep.forest = TRUE)
?rf.crossValidation
rf_cv <- rf.crossValidation(rf, X_train, y_train,
bootstrap=FALSE, xtest=X_val, ytest=y_val,
keep.forest = TRUE)
rf_cv <- rf.crossValidation(rf, X_train, y_train,
bootstrap=FALSE, xtest=data.frame(X_val), ytest=y_val,
keep.forest = TRUE)
print(rf_cv)
rf_predicted <- predict(rf, X_test) %>%
cumsum() %>%
exp()
rf_predicted <- rf_predicted * as.vector(tail(hsbc_ts, 1))
results <- data.frame(cbind(results, rf_predicted))
head(results)
rf_accuracy <- accuracy(rf_predicted, y_test)
?XBGTrainer
?XGBTrainer
xgb <- XGBTrainer$New(objective="reg:squarederror")
xgb <- XGBTrainer$new(objective="reg:squarederror")
xgb_gs <- GridSearchCV$new(trainer=xgb,
parameters=list(learning_rate=seq(from=0.1, to=0.5, by=0.05),
max_depth=seq(from=3, to=10, by=1),
subsample=seq(from=0.4, to=1, by=0.1),
n_estimators=seq(from=50, to=300, by=50)),
n_folds=5,
scoring="rmse")
xgb_gs$fit(hsbc_train, "X1")
xgb_gs$best_iteration()
print(xgb)
?xgboost
xgb_tuned <- xgboost(data=X_train,
label=y_train,
nrounds=10,
params=list(eta=0.5,
max_depth=8,
subsample=0.4,
n_estimators=250))
xgb_tuned <- xgboost(data=X_train,
label=y_train,
nrounds=120,
params=list(eta=0.5,
max_depth=8,
subsample=0.4,
n_estimators=250))
rf_cv_predicted <- predict(rf, X_test)
linear_predicted <- predict.lm(linear, newdata = data.frame(hsbc_test)) %>%
cumsum() %>%
exp()
linear_predicted <- linear_predicted * as.vector(tail(hsbc_ts, 1))
results <- data.frame(cbind(y_test, linear_predicted))
names(results)[1] <- "actual"
linear_accuracy <- accuracy(linear_predicted, y_test)
rf_cv_predicted <- predict(rf, X_test) %>%
cumsum() %>%
exp()
rf_cv_predicted <- rf_cv_predicted * as.vector(tail(hsbc_ts, 1))
results <- data.frame(cbind(results, rf_cv_predicted))
head(results)
rf_accuracy <- accuracy(rf_cv_predicted, y_test)
xgb_tuned_predicted <- predict(xgb, X_test) %>%
cumsum() %>%
exp()
xgb_tuned_predicted <- predict(xgb_tuned, X_test) %>%
cumsum() %>%
exp()
xgb_tuned_predicted <-xgb_tuned_predicted * as.vector(tail(hsbc_ts, 1))
results <- data.frame(cbind(results, xgb_predicted))
results <- data.frame(cbind(results, xgb_tuned_predicted))
head(results)
xgb_accuracy <- accuracy(xgb_tuned_predicted, y_test)
print(xgb_accuracy)
?accuracy
print(rf_accuracy)
print(linear_accuracy)
?accuracy
?forecast$accuracy
results <- data.frame(cbind(results, xgb_tuned_predicted))
head(results)
xgb_accuracy <- forecast::accuracy(xgb_tuned_predicted, y_test)
xgb_accuracy
linear_predicted <- predict.lm(linear, newdata = data.frame(hsbc_test)) %>%
cumsum() %>%
exp()
linear_predicted <- linear_predicted * as.vector(tail(hsbc_ts, 1))
results <- data.frame(cbind(y_test, linear_predicted))
names(results)[1] <- "actual"
linear_accuracy <- forecast::accuracy(linear_predicted, y_test)
rf_cv_predicted <- predict(rf, X_test) %>%
cumsum() %>%
exp()
rf_cv_predicted <- rf_cv_predicted * as.vector(tail(hsbc_ts, 1))
results <- data.frame(cbind(results, rf_cv_predicted))
head(results)
rf_accuracy <- forecast::accuracy(rf_cv_predicted, y_test)
xgb_tuned <- xgboost(data=X_train,
label=y_train,
nrounds=50,
params=list(eta=0.5,
max_depth=8,
subsample=0.4,
n_estimators=250))
xgb_tuned_predicted <- predict(xgb_tuned, X_test) %>%
cumsum() %>%
exp()
xgb_tuned_predicted <-xgb_tuned_predicted * as.vector(tail(hsbc_ts, 1))
xgb_accuracy
xgb_tuned <- xgboost(data=X_train,
label=y_train,
nrounds=10,
params=list(eta=0.5,
max_depth=8,
subsample=0.4,
n_estimators=250))
xgb_tuned_predicted <- predict(xgb_tuned, X_test) %>%
cumsum() %>%
exp()
xgb_tuned_predicted <-xgb_tuned_predicted * as.vector(tail(hsbc_ts, 1))
xgb_accuracy <- forecast::accuracy(xgb_tuned_predicted, y_test)
xgb_accuracy
xgb_tuned <- xgboost(data=X_train,
label=y_train,
nrounds=100,
params=list(eta=0.5,
max_depth=8,
subsample=0.4,
n_estimators=250))
xgb_tuned_predicted <- predict(xgb_tuned, X_test) %>%
cumsum() %>%
exp()
xgb_tuned_predicted <-xgb_tuned_predicted * as.vector(tail(hsbc_ts, 1))
xgb_accuracy <- forecast::accuracy(xgb_tuned_predicted, y_test)
xgb_accuracy
xgb_tuned <- xgboost(data=X_train,
label=y_train,
nrounds=120,
params=list(eta=0.5,
max_depth=8,
subsample=0.4,
n_estimators=250))
xgb_tuned_predicted <- predict(xgb_tuned, X_test) %>%
cumsum() %>%
exp()
xgb_tuned_predicted <-xgb_tuned_predicted * as.vector(tail(hsbc_ts, 1))
xgb_accuracy <- forecast::accuracy(xgb_tuned_predicted, y_test)
xgb_accuracy
xgb_tuned <- xgboost(data=X_train,
label=y_train,
nrounds=150,
params=list(eta=0.5,
max_depth=8,
subsample=0.4,
n_estimators=250))
xgb_tuned_predicted <- predict(xgb_tuned, X_test) %>%
cumsum() %>%
exp()
xgb_tuned_predicted <-xgb_tuned_predicted * as.vector(tail(hsbc_ts, 1))
xgb_accuracy <- forecast::accuracy(xgb_tuned_predicted, y_test)
xgb_accuracy
xgb_tuned <- xgboost(data=X_train,
label=y_train,
nrounds=120,
params=list(eta=0.5,
max_depth=8,
subsample=0.4,
n_estimators=250))
xgb_tuned_predicted <- predict(xgb_tuned, X_test) %>%
cumsum() %>%
exp()
xgb_tuned_predicted <-xgb_tuned_predicted * as.vector(tail(hsbc_ts, 1))
xgb_accuracy <- forecast::accuracy(xgb_tuned_predicted, y_test)
xgb_accuracy
results <- data.frame(cbind(results, xgb_tuned_predicted))
head(results)
xgb_accuracy <- forecast::accuracy(xgb_tuned_predicted, y_test)
?SVMTrainer
?SVMTrainer
?superml
GridSearchCV
?GridSearchCV
?SVMTrainer
??SVMTrainer
svm <- SVMTrainer$new()
svm <- SVMTrainer$new()
# Run once for installation
# Quandl API key: rTCfeqfv2pbfQG-uk-af
# install.packages("Quandl")
# install.packages("keras")
# install_keras()
# install.packages("tensorflow")
# install.packages("tsibble")
# install.packages("randomforest")
# install.packages("forecast")
# install.packages("aTSA)
# install.packages("xgboost")
# install.packages("superml")
# install.packages("rfUtilities")
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse) # For loading the generally needed packages, dplyr, ggplot2 etc
library(tsibble) # Extension of the tidyverse to time-series data
library(Quandl) # API for downloading the data
library(keras) # To train the LSTM model
library(randomForest) # To implement random forests
library(forecast) # To assess predictions and compare them to the real test set
library(aTSA)
library(xgboost)
library(e1071)
library(superml)
library(lmtest)
library(rfUtilities)
svm <- SVMTrainer$new()
# Run once for installation
# Quandl API key: rTCfeqfv2pbfQG-uk-af
# install.packages("Quandl")
# install.packages("keras")
# install_keras()
# install.packages("tensorflow")
# install.packages("tsibble")
# install.packages("randomforest")
# install.packages("forecast")
# install.packages("aTSA)
# install.packages("xgboost")
# install.packages("superml")
# install.packages("rfUtilities")
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse) # For loading the generally needed packages, dplyr, ggplot2 etc
library(tsibble) # Extension of the tidyverse to time-series data
library(Quandl) # API for downloading the data
library(keras) # To train the LSTM model
library(randomForest) # To implement random forests
library(forecast) # To assess predictions and compare them to the real test set
library(aTSA)
library(xgboost)
library(e1071)
library(superml)
library(lmtest)
library(rfUtilities)
library(caret)
set.seed(177125)
lstm %>% compile(optimizer="nadam", loss="mean_squared_error")
lstm <- keras_model_sequential() %>%
layer_lstm(units=32,
activation="tanh", #Default actiavtion for LSTM
return_sequences = FALSE, #Only one LSTM layer hence sequence of outputs not needed
input_shape = dim(X_train_lstm)[-1]) %>% #The layer takes as an input 20 timesteps (lags) and 1 feature
layer_dropout(0.2) %>%
layer_dense(units=1) #Linear activation used rather than RELu/SELu as output can be positive or negative
lstm %>% compile(optimizer="nadam", loss="mean_squared_error")
lstm_model <- lstm %>% fit(x=X_train_lstm,
y=y_train,
batch_size=32,
epochs=20,
validation_data=val_list,
shuffle=FALSE)
lstm_predicted <- predict(lstm, X_test_lstm) %>%
cumsum() %>%
exp()
lstm_predicted <-lstm_predicted * as.vector(tail(hsbc_ts, 1))
lstm_predicted <- predict(lstm, X_test_lstm) %>%
cumsum() %>%
exp()
lstm_predicted <-lstm_predicted * as.vector(tail(hsbc_ts, 1))
lstm_accuracy <- forecast::accuracy(lstm_predicted, y_test)
print(lstm_accuracy)
lstm_model <- lstm %>% fit(x=X_train_lstm,
y=y_train,
batch_size=32,
epochs=50,
validation_data=val_list,
shuffle=FALSE)
lstm %>% compile(optimizer="nadam", loss="mean_squared_error")
lstm_model <- lstm %>% fit(x=X_train_lstm,
y=y_train,
batch_size=32,
epochs=50,
validation_data=val_list,
shuffle=FALSE)
lstm_predicted <- predict(lstm, X_test_lstm) %>%
cumsum() %>%
exp()
lstm_predicted <-lstm_predicted * as.vector(tail(hsbc_ts, 1))
lstm_accuracy <- forecast::accuracy(lstm_predicted, y_test)
print(lstm_accuracy)
lstm_model <- lstm %>% fit(x=X_train_lstm,
y=y_train,
batch_size=32,
epochs=100,
validation_data=val_list,
shuffle=FALSE)
lstm_predicted <- predict(lstm, X_test_lstm) %>%
cumsum() %>%
exp()
lstm_predicted <-lstm_predicted * as.vector(tail(hsbc_ts, 1))
lstm_accuracy <- forecast::accuracy(lstm_predicted, y_test)
print(lstm_accuracy)
lstm_model <- lstm %>% fit(x=X_train_lstm,
y=y_train,
batch_size=32,
epochs=75,
validation_data=val_list,
shuffle=FALSE)
lstm_predicted <- predict(lstm, X_test_lstm) %>%
cumsum() %>%
exp()
lstm_predicted <-lstm_predicted * as.vector(tail(hsbc_ts, 1))
lstm_accuracy <- forecast::accuracy(lstm_predicted, y_test)
print(lstm_accuracy)
lstm %>% compile(optimizer="nadam", loss="mean_squared_error")
lstm_model <- lstm %>% fit(x=X_train_lstm,
y=y_train,
batch_size=32,
epochs=50,
validation_data=val_list,
shuffle=FALSE)
lstm_predicted <- predict(lstm, X_test_lstm) %>%
cumsum() %>%
exp()
lstm_predicted <-lstm_predicted * as.vector(tail(hsbc_ts, 1))
lstm_accuracy <- forecast::accuracy(lstm_predicted, y_test)
print(lstm_accuracy)
plot(lstm_model)
?evaluate.keras.engine.training.Model
?cat
?tuning_run
lstm_parameters <- list(units=seq(from=1, to=32, by=1)
dropout=seq(from=0.1, to=0.9, by=0.1))
lstm_parameters <- list(units=seq(from=1, to=32, by=1),
dropout=seq(from=0.1, to=0.9, by=0.1))
lstm_model <- lstm %>% fit(x=X_train_lstm,
y=y_train,
batch_size=32,
epochs=100,
validation_data=val_list,
shuffle=FALSE)
?ls_runs
runs <- tuning_runs("lstm.R", flags=lstm_parameters, sample=0.5)
# Run once for installation
# Quandl API key: rTCfeqfv2pbfQG-uk-af
# install.packages("Quandl")
# install.packages("keras")
# install_keras()
# install.packages("tensorflow")
# install.packages("tsibble")
# install.packages("randomforest")
# install.packages("forecast")
# install.packages("aTSA)
# install.packages("xgboost")
# install.packages("superml")
# install.packages("rfUtilities")
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse) # For loading the generally needed packages, dplyr, ggplot2 etc
library(tsibble) # Extension of the tidyverse to time-series data
library(Quandl) # API for downloading the data
library(keras) # To train the LSTM model
library(randomForest) # To implement random forests
library(forecast) # To assess predictions and compare them to the real test set
library(aTSA)
library(xgboost)
library(e1071)
library(superml)
library(lmtest)
library(rfUtilities)
library(tfruns)
set.seed(177125)
runs <- tuning_runs("lstm.R", flags=lstm_parameters, sample=0.5)
# Run once for installation
# Quandl API key: rTCfeqfv2pbfQG-uk-af
# install.packages("Quandl")
# install.packages("keras")
# install_keras()
# install.packages("tensorflow")
# install.packages("tsibble")
# install.packages("randomforest")
# install.packages("forecast")
# install.packages("aTSA)
# install.packages("xgboost")
# install.packages("superml")
# install.packages("rfUtilities")
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse) # For loading the generally needed packages, dplyr, ggplot2 etc
library(tsibble) # Extension of the tidyverse to time-series data
library(Quandl) # API for downloading the data
library(keras) # To train the LSTM model
library(randomForest) # To implement random forests
library(forecast) # To assess predictions and compare them to the real test set
library(aTSA)
library(xgboost)
library(e1071)
library(superml)
library(lmtest)
library(rfUtilities)
set.seed(177125)
runs <- tuning_run("lstm.R", flags=lstm_parameters, sample=0.5)
runs <- tuning_run("lstm.R", flags=lstm_parameters, sample=0.5)
runs <- tuning_run("lstm.R", flags=lstm_parameters, sample=0.5)
runs <- tuning_run("lstm.R", flags=lstm_parameters, sample=0.5)
lstm_parameters <- list(units=seq(from=4, to=16, by=2),
dropout=seq(from=0.2, to=0.6, by=0.1))
runs <- tuning_run("lstm.R", flags=lstm_parameters, sample=0.5)
ls_runs(order = metric_val_mean_squared_error, decreasing= FALSE)
ls_runs()
ls_runs(order = metric_val_loss, decreasing= FALSE)
?training_run
best_run <- ls_runs(order=metric_val_loss, decreasing=FALSE)[1,]
run <- training_run('lstm.R',flags = list(dropout = best_run$flag_dropout,
units = best_run$flag_units))
best_model <- load_model_hdf5('lstm.h5')
summary(best_model)
best_model %>% compile(optimizer="nadam", loss="mean_squared_error")
best_model_training <- best_model %>% fit(x=X_train_lstm,
y=y_train,
batch_size=32,
epochs=100,
validation_data=val_list,
shuffle=FALSE)
plot(best_model_training)
lstm_predicted <- predict(best_model, X_test_lstm) %>%
cumsum() %>%
exp()
lstm_predicted <-lstm_predicted * as.vector(tail(hsbc_ts, 1))
lstm_accuracy <- forecast::accuracy(lstm_predicted, y_test)
print(lstm_accuracy)

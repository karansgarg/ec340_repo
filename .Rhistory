linear_accuracy <- forecast::accuracy(linear_predicted, y_test)
linear_accuracy
?GridSearchCV
# Run once for installation
# Quandl API key: rTCfeqfv2pbfQG-uk-af
# install.packages("Quandl")
# install.packages("keras")
# install_keras()
# install.packages("tensorflow")
# install.packages("tsibble")
# install.packages("randomforest")
# install.packages("forecast")
# install.packages("aTSA)
# install.packages("xgboost")
# install.packages("superml")
# install.packages("rfUtilities")
# install.packages("gridExtra")
# install.packages("ggplotify")
knitr::opts_chunk$set(echo=TRUE, message=FALSE, results='hide')
library(tidyverse) # For loading the generally needed packages, dplyr, ggplot2 etc
library(tsibble) # Extension of the tidyverse to time-series data
library(Quandl) # API for downloading the data
library(keras) # For Artificial Neural Network
library(randomForest) # To implement random forests
library(forecast) # To assess predictions and compare them to the real test set
library(aTSA) #Useful for time series analysis
library(xgboost) # For gradient boosted random forests
#library(e1071) # For
library(superml) # For cross validation of the XGB model
library(lmtest) # For testing the basic, linear model
library(rfUtilities) #For cross validation of the random forest
library(gridExtra)
library(ggplotify)
library(glmnet)
set.seed(177125)
# For complete files, including lstm.R and lstm_log_diff.R, please see: https://github.com/karansgarg/ec340_repo
results_truncated <- results[1:350, ]
trunc_accuracies <- rbind((forecast::accuracy(results$linear_predicted, results_truncated$actual)),
(forecast::accuracy(results$linear_log_diff_predicted, results_truncated$actual)),
(forecast::accuracy(results_truncated$rf_cv_predicted, results_truncated$actual)),
(forecast::accuracy(results_truncated$rf_cv_log_diff_predicted, results_truncated$actual)),
(forecast::accuracy(results_truncated$xgb_norm_predicted, results_truncated$actual)),
(forecast::accuracy(results_truncated$xgb_log_diff_predicted, results_truncated$actual)),
(forecast:: accuracy(results_truncated$V5, results_truncated$actual)),
(forecast::accuracy(results_truncated$lstm_log_diff_predicted, results_truncated$actual)))
rownames(trunc_accuracies) <- c("Linear Model w/ Normalised Data",
"Linear Model w/ Differenced Data",
"Random Forest w/ Normalised Data",
"Random Forest w/ Differenced Data",
"Gradient Boosted Random Forest w/ Normalised Data",
"Gradient Boosted Random Forest w/ Differenced Data",
"Recurrent Neural Network w/ Normalised Data",
"Recurrent Neural Network w/ Differenced Data")
print(trunc_accuracies)
ggplot(data=results_truncated) +
geom_line(mapping=aes(x=timesteps, y=actual, colour="Actual")) +
geom_line(mapping=aes(x=timesteps, y=linear_predicted, colour="Linear")) +
geom_line(mapping=aes(x=timesteps, y=rf_cv_predicted, colour="Random Forest")) +
geom_line(mapping=aes(x=timesteps, y=xgb_norm_predicted, colour="XGB")) +
geom_line(mapping=aes(x=timesteps, y=V5, colour="ANN"))
knitr::opts_chunk$set(echo = TRUE)
# Run once for installation
# Quandl API key: rTCfeqfv2pbfQG-uk-af
# install.packages("Quandl")
# install.packages("keras")
# install_keras()
# install.packages("tensorflow")
# install.packages("tsibble")
# install.packages("randomforest")
# install.packages("forecast")
# install.packages("aTSA)
# install.packages("xgboost")
# install.packages("superml")
# install.packages("rfUtilities")
knitr::opts_chunk$set(echo=TRUE, message=FALSE, results='hide')
library(tidyverse) # For loading the generally needed packages, dplyr, ggplot2 etc
library(tsibble) # Extension of the tidyverse to time-series data
library(Quandl) # API for downloading the data
library(keras) # To train the LSTM model
library(randomForest) # To implement random forests
library(forecast) # To assess predictions and compare them to the real test set
library(aTSA)
library(xgboost)
library(e1071)
library(superml)
library(lmtest)
library(rfUtilities)
set.seed(177125)
test1 <- XGBTrainer$new(objective="reg:squarederror")
test2 <- GridSearchCV$new(trainer=test1,
parameters=list(learning_rate=seq(from=0.1, to=0.5, by=0.1)),
n_folds=5,
scoring="rmse")
test2$fit(data.matrix(c(1,0,0), c(0,1,0)), "X1")
test1 <- XGBTrainer$new(objective="reg:squarederror")
test2 <- GridSearchCV$new(trainer=test1,
parameters=list(learning_rate=seq(from=0.1, to=0.5, by=0.1)),
n_folds=5,
scoring="rmse")
test2$fit(data.matrix(c(1,0,0), c(0,1,0)))
results <- data.frame(cbind(y_test,
linear_predicted,
rf_cv_predicted,
xgb_norm_predicted,
lstm_norm_predicted))
names(results)[1] <- "actual"
results <- results[-1, ]
results <- data.frame(cbind(results,
linear_log_diff_predicted,
rf_cv_log_diff_predicted,
xgb_log_diff_predicted,
lstm_log_diff_predicted))
results$timesteps <- as.numeric(row.names(results))
accuracies <- data.frame(rbind(linear_accuracy,
linear_log_diff_accuracy,
rf_accuracy,
rf_log_diff_accuracy,
xgb_accuracy,
xgb_log_diff_accuracy,
lstm_norm_accuracy,
lstm_log_diff_accuracy))
rownames(accuracies) <- c("Linear Model w/ Normalised Data",
"Linear Model w/ Differenced Data",
"Random Forest w/ Normalised Data",
"Random Forest w/ Differenced Data",
"Gradient Boosted Decision Trees w/ Normalised Data",
"Gradient Boosted Decision Trees w/ Differenced Data",
"Recurrent Neural Network w/ Normalised Data",
"Recurrent Neural Network w/ Differenced Data")
ggplot(data=results) +
geom_line(mapping=aes(x=timesteps, y=actual, colour="Actual")) +
geom_line(mapping=aes(x=timesteps, y=linear_predicted, colour="Linear")) +
geom_line(mapping=aes(x=timesteps, y=rf_cv_predicted, colour="Random Forest")) +
geom_line(mapping=aes(x=timesteps, y=xgb_norm_predicted, colour="XGB")) +
geom_line(mapping=aes(x=timesteps, y=lstm_norm_predicted[-1], colour="ANN")) +
labs(title = "Model Predictions on Test Set", x = "Timesteps", y = "Price")
results <- data.frame(cbind(y_test,
linear_predicted,
rf_cv_predicted,
xgb_norm_predicted,
lstm_norm_predicted))
names(results)[1] <- "actual"
results <- results[-1, ]
results <- data.frame(cbind(results,
linear_log_diff_predicted,
rf_cv_log_diff_predicted,
xgb_log_diff_predicted,
lstm_log_diff_predicted))
results$timesteps <- as.numeric(row.names(results))
accuracies <- data.frame(rbind(linear_accuracy,
linear_log_diff_accuracy,
rf_accuracy,
rf_log_diff_accuracy,
xgb_accuracy,
xgb_log_diff_accuracy,
lstm_norm_accuracy,
lstm_log_diff_accuracy))
rownames(accuracies) <- c("Linear Model w/ Normalised Data",
"Linear Model w/ Differenced Data",
"Random Forest w/ Normalised Data",
"Random Forest w/ Differenced Data",
"Gradient Boosted Decision Trees w/ Normalised Data",
"Gradient Boosted Decision Trees w/ Differenced Data",
"Recurrent Neural Network w/ Normalised Data",
"Recurrent Neural Network w/ Differenced Data")
ggplot(data=results) +
geom_line(mapping=aes(x=timesteps, y=actual, colour="Actual")) +
geom_line(mapping=aes(x=timesteps, y=linear_predicted, colour="Linear")) +
geom_line(mapping=aes(x=timesteps, y=rf_cv_predicted, colour="Random Forest")) +
geom_line(mapping=aes(x=timesteps, y=xgb_norm_predicted, colour="XGB")) +
geom_line(mapping=aes(x=timesteps, y=lstm_norm_predicted[-1], colour="ANN")) +
labs(title = "Model Predictions on Test Set", x = "Timesteps", y = "Price", colour="Model")
results_truncated <- results[1:350, ]
trunc_accuracies <- rbind((forecast::accuracy(results$linear_predicted, results_truncated$actual)),
(forecast::accuracy(results$linear_log_diff_predicted, results_truncated$actual)),
(forecast::accuracy(results_truncated$rf_cv_predicted, results_truncated$actual)),
(forecast::accuracy(results_truncated$rf_cv_log_diff_predicted, results_truncated$actual)),
(forecast::accuracy(results_truncated$xgb_norm_predicted, results_truncated$actual)),
(forecast::accuracy(results_truncated$xgb_log_diff_predicted, results_truncated$actual)),
(forecast:: accuracy(results_truncated$V5, results_truncated$actual)),
(forecast::accuracy(results_truncated$lstm_log_diff_predicted, results_truncated$actual)))
rownames(trunc_accuracies) <- c("Linear Model w/ Normalised Data",
"Linear Model w/ Differenced Data",
"Random Forest w/ Normalised Data",
"Random Forest w/ Differenced Data",
"Gradient Boosted Decision Trees w/ Normalised Data",
"Gradient Boosted Decision Trees w/ Differenced Data",
"Recurrent Neural Network w/ Normalised Data",
"Recurrent Neural Network w/ Differenced Data")
print(trunc_accuracies)
#ggplot(data=results_truncated) +
#  geom_line(mapping=aes(x=timesteps, y=actual, colour="Actual")) +
#  geom_line(mapping=aes(x=timesteps, y=linear_predicted, colour="Linear")) +
#  geom_line(mapping=aes(x=timesteps, y=rf_cv_predicted, colour="Random Forest")) +
#  geom_line(mapping=aes(x=timesteps, y=xgb_norm_predicted, colour="Gradient Boost")) +
#  geom_line(mapping=aes(x=timesteps, y=V5, colour="LSTM"))
results_truncated <- results[1:350, ]
trunc_accuracies <- data.frame(rbind((forecast::accuracy(results$linear_predicted, results_truncated$actual)),
(forecast::accuracy(results$linear_log_diff_predicted, results_truncated$actual)),
(forecast::accuracy(results_truncated$rf_cv_predicted, results_truncated$actual)),
(forecast::accuracy(results_truncated$rf_cv_log_diff_predicted, results_truncated$actual)),
(forecast::accuracy(results_truncated$xgb_norm_predicted, results_truncated$actual)),
(forecast::accuracy(results_truncated$xgb_log_diff_predicted, results_truncated$actual)),
(forecast:: accuracy(results_truncated$V5, results_truncated$actual)),
(forecast::accuracy(results_truncated$lstm_log_diff_predicted, results_truncated$actual))))
rownames(trunc_accuracies) <- c("Linear Model w/ Normalised Data",
"Linear Model w/ Differenced Data",
"Random Forest w/ Normalised Data",
"Random Forest w/ Differenced Data",
"Gradient Boosted Decision Trees w/ Normalised Data",
"Gradient Boosted Decision Trees w/ Differenced Data",
"Recurrent Neural Network w/ Normalised Data",
"Recurrent Neural Network w/ Differenced Data")
print(trunc_accuracies)
#ggplot(data=results_truncated) +
#  geom_line(mapping=aes(x=timesteps, y=actual, colour="Actual")) +
#  geom_line(mapping=aes(x=timesteps, y=linear_predicted, colour="Linear")) +
#  geom_line(mapping=aes(x=timesteps, y=rf_cv_predicted, colour="Random Forest")) +
#  geom_line(mapping=aes(x=timesteps, y=xgb_norm_predicted, colour="Gradient Boost")) +
#  geom_line(mapping=aes(x=timesteps, y=V5, colour="LSTM"))
length(hsbc_ts)
nrow(hsbc_lagged)
split_num_val
# Run once for installation
# Quandl API key: rTCfeqfv2pbfQG-uk-af
# install.packages("Quandl")
# install.packages("keras")
# install_keras()
# install.packages("tensorflow")
# install.packages("tsibble")
# install.packages("randomforest")
# install.packages("forecast")
# install.packages("aTSA)
# install.packages("xgboost")
# install.packages("superml")
# install.packages("rfUtilities")
# install.packages("gridExtra")
# install.packages("ggplotify")
knitr::opts_chunk$set(echo=TRUE, message=FALSE, results='hide')
library(tidyverse) # For loading the generally needed packages, dplyr, ggplot2 etc
library(tsibble) # Extension of the tidyverse to time-series data
library(Quandl) # API for downloading the data
library(keras) # For Artificial Neural Network
library(randomForest) # To implement random forests
library(forecast) # To assess predictions and compare them to the real test set
library(aTSA) #Useful for time series analysis
library(xgboost) # For gradient boosted random forests
library(superml) # For cross validation of the XGB model
library(lmtest) # For testing the basic, linear model
library(rfUtilities) #For cross validation of the random forest
library(glmnet) # For selecting the relevant lag length in training the models
set.seed(177125)
# For complete files, including lstm.R and lstm_log_diff.R, please see: https://github.com/karansgarg/ec340_repo
hsbc_ts_split <- hsbc_ts(41:1858)
hsbc_ts_split <- hsbc_ts[41:1858]
hsbc_ts_split_norm <- scale(log(hsbc_ts_norm))
hsbc_ts_split_norm <- scale(log(hsbc_ts_split))
Norm <- function(data){
data <- (data - attr(hsbc_ts_split_norm, "scaled:center"))/attr(hsbc_ts_split_norm, "scaled:scale")
return(data)
}
ReverseNorm <- function(data){
data <- (data*attr(hsbc_ts_split_norm, "scaled:scale") + attr(hsbc_ts_split_norm, "scaled:center")) %>%
exp()
return(data)
}
hsbc_normalised <- Norm(log(hsbc_ts))
# Using the inbuilt embedding function, lags of the series are generated to be used as features.
# Both the stationary, and non-stationary series are embedded as both will be used for analysis.
hsbc_ts_split <- hsbc_ts[41:1858]
hsbc_ts_split_norm <- scale(log(hsbc_ts_split))
Norm <- function(data){
data <- (data - attr(hsbc_ts_split_norm, "scaled:center"))/attr(hsbc_ts_split_norm, "scaled:scale")
return(data)
}
ReverseNorm <- function(data){
data <- (data*attr(hsbc_ts_split_norm, "scaled:scale") + attr(hsbc_ts_split_norm, "scaled:center")) %>%
exp()
return(data)
}
hsbc_normalised <- Norm(log(hsbc_ts))
hsbc_lagged <- embed(hsbc_normalised, 41)
hsbc_log_diff_lagged <- embed(hsbc_log_diff, 41)
split_num_val <- round(nrow(hsbc_lagged)*0.7) # 10% of data reserved for validation
split_num_train <- round(nrow(hsbc_lagged)*0.6) #30% of data reserved for testing
hsbc_train <- data.frame(hsbc_lagged[1: split_num_train, ])
hsbc_val <- data.frame(hsbc_lagged[(split_num_train+1):split_num_val, ])
hsbc_test <- data.frame(hsbc_lagged[(split_num_val+1):nrow(hsbc_lagged), ])
hsbc_log_diff_train <- data.frame(hsbc_log_diff_lagged[1: split_num_train, ])
hsbc_log_diff_val <- data.frame(hsbc_log_diff_lagged[(split_num_train+1):split_num_val, ])
hsbc_log_diff_test <- data.frame(hsbc_log_diff_lagged[(split_num_val+1):nrow(hsbc_log_diff_lagged), ])
X_train <- data.matrix(hsbc_train[, -1])
X_train_lstm <- array(X_train, c(dim(X_train), 1))
y_train <- hsbc_train[, 1]
X_log_diff_train <- data.matrix(hsbc_log_diff_train[, -1])
X_log_diff_train_lstm <- array(X_log_diff_train, c(dim(X_log_diff_train), 1))
y_log_diff_train <- hsbc_log_diff_train[, 1]
X_val <- data.matrix(hsbc_val[, -1])
X_val_lstm <- array(X_val, c(dim(X_val), 1))
y_val <- hsbc_val[, 1]
val_list <- list(X_val_lstm, y_val)
X_log_diff_val <- data.matrix(hsbc_log_diff_val[, -1])
X_log_diff_val_lstm <- array(X_log_diff_val, c(dim(X_log_diff_val), 1))
y_log_diff_val <- hsbc_log_diff_val[, 1]
val_log_diff_list <- list(X_log_diff_val_lstm, y_log_diff_val)
X_test <- data.matrix(hsbc_test[, -1])
X_test_lstm <- array(X_test, c(dim(X_test), 1))
y_test <- hsbc_test[, 1] %>%
ReverseNorm()
X_log_diff_test <- data.matrix(hsbc_log_diff_test[, -1])
X_log_diff_test_lstm <- array(X_log_diff_test, c(dim(X_log_diff_test), 1))
y_log_diff_test <- hsbc_log_diff_test[, 1] %>%
ReverseLogDiff()
# Fit both linear models and record their predictions and accuracy.
linear_norm <- lm(X1 ~ X2 + X3 + X4 + X5 + X6 + X7 + X8 + X9 + X10 + X11 + X12 + X13 + X14 + X15 + X16 + X17 + X18 + X19 + X20 + X21 + X22 + X23 + X24 + X25 + X26 + X27 + X28 + X29 + X30 + X31 + X32 + X33 + X34 + X35 + X36 + X37 + X38 + X39 + X40 + X41, data=hsbc_train)
summary(linear_norm)
resettest(linear_norm)
linear_predicted <- predict.lm(linear_norm, newdata = data.frame(hsbc_test)) %>%
ReverseNorm()
linear_accuracy <- forecast::accuracy(linear_predicted, y_test)
linear_accuracy
linear_log_diff <- lm(X1 ~ X2 + X3 + X4 + X5 + X6 + X7 + X8 + X9 + X10 + X11 + X12 + X13 + X14 + X15 + X16 + X17 + X18 + X19 + X20 + X21 + X22 + X23 + X24 + X25 + X26 + X27 + X28 + X29 + X30 + X31 + X32 + X33 + X34 + X35 + X36 + X37 + X38 + X39 + X40 + X41, data=hsbc_log_diff_train)
summary(linear_log_diff)
resettest(linear_log_diff)
linear_log_diff_predicted <- predict.lm(linear_log_diff, newdata = data.frame(hsbc_log_diff_test)) %>%
ReverseNorm()
linear_log_diff_accuracy <- forecast::accuracy(linear_log_diff_predicted, y_log_diff_test)
linear_log_diff_accuracy
# Fit both random forest models (after cross validation) and record predictions and accuracy.
rf <- randomForest(X_train, y_train,
xtest=X_val, ytest=y_val,
keep.forest = TRUE)
rf_cv <- rf.crossValidation(rf, X_train, y_train,
bootstrap=FALSE, xtest=data.frame(X_val), ytest=y_val,
keep.forest = TRUE)
rf_cv_predicted <- predict(rf, X_test) %>%
ReverseNorm()
rf_accuracy <- forecast::accuracy(rf_cv_predicted, y_test)
rf_accuracy
rf_log_diff <- randomForest(X_log_diff_train, y_log_diff_train,
xtest=X_log_diff_val, ytest=y_log_diff_val,
keep.forest = TRUE)
rf_cv_log_diff <- rf.crossValidation(rf_log_diff, X_log_diff_train, y_log_diff_train,
bootstrap=FALSE, xtest=data.frame(X_log_diff_val), ytest=y_log_diff_val,
keep.forest = TRUE)
rf_cv_log_diff_predicted <- predict(rf_log_diff, X_log_diff_test) %>%
ReverseNorm()
rf_log_diff_accuracy <- forecast::accuracy(rf_cv_log_diff_predicted, y_log_diff_test)
rf_log_diff_accuracy
# Fit both gradient boosted decision tree models (after cross validation) and record predictions and accuracy.
xgb_norm <- XGBTrainer$new(objective="reg:squarederror")
xgb_gs_norm <- GridSearchCV$new(trainer=xgb_norm,
parameters=list(learning_rate=seq(from=0.1, to=0.5, by=0.1),
max_depth=seq(from=3, to=10, by=1),
subsample=seq(from=0.4, to=1, by=0.1),
n_estimators=seq(from=100, to=500, by=100)),
n_folds=5,
scoring="rmse")
xgb_gs_norm$fit(hsbc_train, "X1")
xgb_norm_best <- xgb_gs_norm$best_iteration()
xgb_norm_tuned <- xgboost(data=X_train,
label=y_train,
nrounds=2000,
params=list(eta=xgb_norm_best$learning_rate,
max_depth=xgb_norm_best$max_depth,
subsample=xgb_norm_best$subsample,
n_estimators=xgb_norm_best$n_estimators))
xgb_norm_predicted <- predict(xgb_norm_tuned, X_test) %>%
ReverseNorm()
xgb_accuracy <- forecast::accuracy(xgb_norm_predicted, y_test)
xgb_accuracy
xgb_log_diff <- XGBTrainer$new(objective="reg:squarederror")
xgb_gs_log_diff <- GridSearchCV$new(trainer=xgb_log_diff,
parameters=list(learning_rate=seq(from=0.1, to=0.5, by=0.1),
max_depth=seq(from=3, to=10, by=1),
subsample=seq(from=0.4, to=1, by=0.1),
n_estimators=seq(from=100, to=500, by=100)),
n_folds=5,
scoring="rmse")
xgb_gs_log_diff$fit(hsbc_log_diff_train, "X1")
xgb_log_diff_best <- xgb_gs_log_diff$best_iteration()
xgb_log_diff_tuned <- xgboost(data=X_log_diff_train,
label=y_log_diff_train,
nrounds=200,
params=list(eta=xgb_log_diff_best$learning_rate,
max_depth=xgb_log_diff_best$max_depth,
subsample=xgb_log_diff_best$subsample,
n_estimators=xgb_log_diff_best$n_estimators))
xgb_log_diff_predicted <- predict(xgb_log_diff_tuned, X_log_diff_test) %>%
ReverseNorm()
xgb_log_diff_accuracy <- forecast::accuracy(xgb_log_diff_predicted, y_log_diff_test)
xgb_log_diff_accuracy
# Tune the hyperparameters for the models, fit them, and record predictions and accuracies.
# Note this chunk contains code from "lstm.R" and "lstm_log_diff.R", both of which can be found at the link below.
# https://github.com/karansgarg/ec340_repo
lstm_parameters <- list(units=seq(from=4, to=16, by=2),
dropout=seq(from=0.2, to=0.6, by=0.1))
runs <- tfruns::tuning_run("lstm.R", flags=lstm_parameters, sample=0.5)
tfruns::ls_runs(order=metric_val_loss, decreasing=FALSE)
best_run <- tfruns::ls_runs(order=metric_val_loss, decreasing=FALSE)[1,]
run <- tfruns::training_run('lstm.R',flags = list(dropout = best_run$flag_dropout,
units = best_run$flag_units))
best_model_norm <- load_model_hdf5('lstm.h5')
best_model_norm %>% compile(optimizer="nadam", loss="mean_squared_error")
best_model_norm_training <- best_model_norm %>% fit(x=X_train_lstm,
y=y_train,
batch_size=128,
epochs=100,
validation_data=val_list,
shuffle=FALSE)
lstm_norm_predicted <- predict(best_model_norm, X_test_lstm) %>%
ReverseNorm()
lstm_norm_accuracy <- forecast::accuracy(as.ts(lstm_norm_predicted), y_test)
print(lstm_norm_accuracy)
runs <- tfruns::tuning_run("lstm_log_diff.R", flags=lstm_parameters, sample=0.5)
tfruns::ls_runs(order=metric_val_loss, decreasing=FALSE)
best_run <- tfruns::ls_runs(order=metric_val_loss, decreasing=FALSE)[1,]
run <- tfruns::training_run('lstm_log_diff.R',flags = list(dropout = best_run$flag_dropout,
units = best_run$flag_units))
best_model_log_diff <- load_model_hdf5('lstm_log_diff.h5')
best_model_log_diff %>% compile(optimizer="nadam", loss="mean_squared_error")
best_model_log_diff_training <- best_model_log_diff %>% fit(x=X_log_diff_train_lstm,
y=y_log_diff_train,
batch_size=128,
epochs=100,
validation_data=val_log_diff_list,
shuffle=FALSE)
lstm_log_diff_predicted <- predict(best_model_log_diff, X_log_diff_test_lstm) %>%
ReverseNorm()
lstm_log_diff_accuracy <- forecast::accuracy(as.ts(lstm_log_diff_predicted), y_log_diff_test)
print(lstm_log_diff_accuracy)
# Aggregate and plot the results.
results <- data.frame(cbind(y_test,
linear_predicted,
rf_cv_predicted,
xgb_norm_predicted,
lstm_norm_predicted))
names(results)[1] <- "actual"
results <- results[-1, ]
results <- data.frame(cbind(results,
linear_log_diff_predicted,
rf_cv_log_diff_predicted,
xgb_log_diff_predicted,
lstm_log_diff_predicted))
results$timesteps <- as.numeric(row.names(results))
accuracies <- data.frame(rbind(linear_accuracy,
linear_log_diff_accuracy,
rf_accuracy,
rf_log_diff_accuracy,
xgb_accuracy,
xgb_log_diff_accuracy,
lstm_norm_accuracy,
lstm_log_diff_accuracy))
rownames(accuracies) <- c("Linear Model w/ Normalised Data",
"Linear Model w/ Differenced Data",
"Random Forest w/ Normalised Data",
"Random Forest w/ Differenced Data",
"Gradient Boosted Decision Trees w/ Normalised Data",
"Gradient Boosted Decision Trees w/ Differenced Data",
"Recurrent Neural Network w/ Normalised Data",
"Recurrent Neural Network w/ Differenced Data")
ggplot(data=results) +
geom_line(mapping=aes(x=timesteps, y=actual, colour="Actual")) +
geom_line(mapping=aes(x=timesteps, y=linear_predicted, colour="Linear")) +
geom_line(mapping=aes(x=timesteps, y=rf_cv_predicted, colour="Random Forest")) +
geom_line(mapping=aes(x=timesteps, y=xgb_norm_predicted, colour="XGB")) +
geom_line(mapping=aes(x=timesteps, y=lstm_norm_predicted[-1], colour="ANN")) +
labs(title = "Model Predictions on Test Set", x = "Timesteps", y = "Price", colour="Model")
# Truncate the results to those observations that are in range of the training data and reassess results.
results_truncated <- results[1:350, ]
trunc_accuracies <- data.frame(rbind((forecast::accuracy(results$linear_predicted, results_truncated$actual)),
(forecast::accuracy(results$linear_log_diff_predicted, results_truncated$actual)),
(forecast::accuracy(results_truncated$rf_cv_predicted, results_truncated$actual)),
(forecast::accuracy(results_truncated$rf_cv_log_diff_predicted, results_truncated$actual)),
(forecast::accuracy(results_truncated$xgb_norm_predicted, results_truncated$actual)),
(forecast::accuracy(results_truncated$xgb_log_diff_predicted, results_truncated$actual)),
(forecast:: accuracy(results_truncated$V5, results_truncated$actual)),
(forecast::accuracy(results_truncated$lstm_log_diff_predicted, results_truncated$actual))))
rownames(trunc_accuracies) <- c("Linear Model w/ Normalised Data",
"Linear Model w/ Differenced Data",
"Random Forest w/ Normalised Data",
"Random Forest w/ Differenced Data",
"Gradient Boosted Decision Trees w/ Normalised Data",
"Gradient Boosted Decision Trees w/ Differenced Data",
"Recurrent Neural Network w/ Normalised Data",
"Recurrent Neural Network w/ Differenced Data")
print(trunc_accuracies)
#ggplot(data=results_truncated) +
#  geom_line(mapping=aes(x=timesteps, y=actual, colour="Actual")) +
#  geom_line(mapping=aes(x=timesteps, y=linear_predicted, colour="Linear")) +
#  geom_line(mapping=aes(x=timesteps, y=rf_cv_predicted, colour="Random Forest")) +
#  geom_line(mapping=aes(x=timesteps, y=xgb_norm_predicted, colour="Gradient Boost")) +
#  geom_line(mapping=aes(x=timesteps, y=V5, colour="LSTM"))
# Create a function to fill missing values in the time series (due to weekends/bank holidays etc).
# Each NA value is replaced with the previous close price of the series. This is the price that market
# participants would see when making trading decisions on days where the exchange was closed.
ImputeMissing <- function(ts){
df <- data.frame(ts)
for(i in 1:nrow(df)){
if(is.na(df[i, 1])){
df[i, 1] <- df[i-1, 1]
}
}
dates_formatted <- data.frame(dates)
df <- cbind(df, dates_formatted)
names(df)[1] <- "Price"
names(df)[2] <- "Date"
tib_new <- as_tsibble(df, index="Date")
return(tib_new)
}
hsbc_tib <- ImputeMissing(hsbc_ts)
hsbc_ts <- as.ts(hsbc_tib, start = c(2014, as.numeric(format(dates[1], "%j"))), frequency=365)

---
title: "Has Data Science Shown Financial Markets are Not Efficient?"
output: html_document
author: "Karan Garg"
---
```{r setup, include=FALSE}
# Run once for installation
# Quandl API key: rTCfeqfv2pbfQG-uk-af
# install.packages("Quandl")
# install.packages("keras")
# install_keras()
# install.packages("tensorflow")
# install.packages("tsibble")
# install.packages("randomforest")
# install.packages("forecast")
# install.packages("aTSA)

knitr::opts_chunk$set(echo = TRUE)
library(tidyverse) # For loading the generally needed packages, dplyr, ggplot2 etc
library(tsibble) # Extension of the tidyverse to time-series data
library(Quandl) # API for downloading the data
library(keras) # To train the LSTM model
library(randomForest) # To implement random forests
library(forecast) # To forecast future prices based on model predictions
library(aTSA)
```
## Introduction

In this paper, I aim to explore the role of data science literature in determining whether or not markets are efficient. Many papers claim to generate supernormal returns with their models (using just past prices), implying weak-form inefficiency, however the type of efficiency is rarely discussed. Fama-EMH efficiency may certainly be going down, but Lo-AMH or Samuelson-EMH efficiency may still be preserved. 2 ML algorithms, random forests and then LSTMs are used to assess whether this is the case.

If the models are able to predict prices well (such that a long-term strategy may be possibly used to generate above-market returns), then that may be evidence markets are not Fama-EMH efficient. However, they may still be Samuelson-EMH efficient if those predictions follow a martingale (best guess for next price is just the previous price), in which case this model is just getting lucky with their results. 
```{r}
print("hello world")
```

## Dataset, Preparation and Feature Engineering

```{r}
#First we download the data from Quandl
#The data is imported as both a tibble and a timeseries object, as both will be useful for analysis

Quandl.api_key('rTCfeqfv2pbfQG-uk-af')
hsbc_tib <- Quandl('HKEX/00005', column_index = "1") %>% 
  as_tsibble(index="Date")

names(hsbc_tib)[2] <- "Price"
dates <- seq(as.Date("2014-02-21"), as.Date("2021-05-05"), by = "day")
hsbc_ts <- as.ts(hsbc_tib, start = c(2014, as.numeric(format(dates[1], "%j"))), frequency=365)

```

```{r}
glimpse(hsbc_tib)
glimpse(hsbc_ts)
```

```{r}
head(hsbc_tib)
head(hsbc_ts)
```

```{r}
has_gaps(hsbc_tib)
colSums(is.na(hsbc_tib[, "Price"]))
```
The data has no missing values, however it does contain gaps as the stock market is closed on weekends, bank holidays, etc and so the stock exchanges do not record a price for those days. In order to prevent this being a problem, the missing values are replaced with the last available price. While this may not reflect the change in price resulting from non-market orders placed over the weekend, it should solve the problem of the missing values without altering the key elements of the data.
```{r}
ImputeMissing <- function(ts){
  df <- data.frame(ts)
  for(i in 1:nrow(df)){
    if(is.na(df[i, 1])){
      df[i, 1] <- df[i-1, 1]
    }
  }
  dates_formatted <- data.frame(dates)
  df <- cbind(df, dates_formatted)
  names(df)[1] <- "Price"
  names(df)[2] <- "Date"
  tib_new <- as_tsibble(df) 
#                  start = c(2014, 32),
#                  frequency=365)
  
  return(tib_new)
}
```

```{r}
hsbc_tib <- ImputeMissing(hsbc_ts)
hsbc_ts <- as.ts(hsbc_tib, start = c(2014, as.numeric(format(dates[1], "%j"))), frequency=365)
```

```{r}
head(hsbc_tib)
```

```{r}

```

```{r}
hsbc_visual <- hsbc_tib %>% 
  ggplot(aes(Date, Price)) +
  geom_line() +
  theme_minimal() +
  labs(title = "HSBC Stock Price", x = "Year", y = "Price")

print(hsbc_visual)
```

```{r}
#acf(hsbc_tib, lag.max = 10, na.action=na.pass)
```

```{r}
adf.test(hsbc_tib$Price)
```
The ADF test revealed that the timeseries is non-stationary. In order to make the data easier for the model to process, we will difference the data and repeat the ADF test. Assuming the timeseries is I(1), a first-difference series should be stationary.


```{r}
hsbc_log_diff <- hsbc_ts %>%
  log() %>%
  diff(1)
```

```{r}
head(hsbc_log_diff)
```

```{r}
plot(hsbc_log_diff)
```
```{r}
adf.test(hsbc_log_diff)
```
We can see from the results, that this first-difference logged series is indeed stationary. 

```{r}
hsbc_lagged <- embed(hsbc_log_diff, 22)
```

```{r}
split_num <- round(nrow(hsbc_lagged)*0.7)
hsbc_train <- hsbc_lagged[1: split_num, ]
hsbc_test <- hsbc_lagged[(split_num+1):nrow(hsbc_lagged), ]
X_train <- hsbc_train[, -1]
y_train <- hsbc_train[, 1]
X_test <- hsbc_test[, -1]
y_test <- hsbc_test[, 1]
```

```{r}
```

## Model and Analysis
Now that we have the data, we can begin to attempt 
```{r}
linear <- lm(y_train ~ X_train)
summary(linear)
```

```{r}
linear_predicted <- predict(linear, newdata = data.frame(X_test))
```
As expected, the standard linear regression has a very low explanatory and indeed predictive power. The lags that do seem to appear significant are likely just a result of the repeated "0" values every weekend (we took the first difference of the data and prices do not change over the weekend as there is no trading).

## Conclusion


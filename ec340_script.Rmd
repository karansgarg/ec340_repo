---
title: "Has Data Science Shown Financial Markets are Not Efficient?"
output: html_document
author: "Karan Garg"
---
```{r setup, include=FALSE}
# Run once for installation
# Quandl API key: rTCfeqfv2pbfQG-uk-af
# install.packages("Quandl")
# install.packages("keras")
# install_keras()
# install.packages("tensorflow")
# install.packages("tsibble")
# install.packages("randomforest")
# install.packages("forecast")
# install.packages("aTSA)
# install.packages("xgboost")

knitr::opts_chunk$set(echo = TRUE)
library(tidyverse) # For loading the generally needed packages, dplyr, ggplot2 etc
library(tsibble) # Extension of the tidyverse to time-series data
library(Quandl) # API for downloading the data
library(keras) # To train the LSTM model
library(randomForest) # To implement random forests
library(forecast) # To assess predictions and compare them to the real test set
library(aTSA)
library(xgboost)
library(e1071)
```
## Introduction

In this paper, I aim to explore the role of data science literature in determining whether or not markets are efficient. Many papers claim to generate supernormal returns with their models (using just past prices), implying weak-form inefficiency, however the type of efficiency is rarely discussed. Fama-EMH efficiency may certainly be going down, but Lo-AMH or Samuelson-EMH efficiency may still be preserved. 2 ML algorithms, random forests and then LSTMs are used to assess whether this is the case.

If the models are able to predict prices well (such that a long-term strategy may be possibly used to generate above-market returns), then that may be evidence markets are not Fama-EMH efficient. However, they may still be Samuelson-EMH efficient if those predictions follow a martingale (best guess for next price is just the previous price), in which case this model is just getting lucky with their results. 
```{r}
#print("hello world")
```

## Dataset, Preparation and Feature Engineering

```{r}
#First we download the data from Quandl
#The data is imported as both a tibble and a timeseries object, as both will be useful for analysis

Quandl.api_key('rTCfeqfv2pbfQG-uk-af')
hsbc_tib <- Quandl('HKEX/00005', column_index = "1") %>% 
  as_tsibble(index="Date")

names(hsbc_tib)[2] <- "Price"
dates <- seq(as.Date("2014-02-21"), as.Date("2021-05-07"), by = "day")
hsbc_ts <- as.ts(hsbc_tib, start = c(2014, as.numeric(format(dates[1], "%j"))), frequency=365)

```

```{r}
glimpse(hsbc_tib)
glimpse(hsbc_ts)
```

```{r}
head(hsbc_tib)
head(hsbc_ts)
```

```{r}
has_gaps(hsbc_tib)
colSums(is.na(hsbc_tib[, "Price"]))
```
The data has no missing values, however it does contain gaps as the stock market is closed on weekends, bank holidays, etc and so the stock exchanges do not record a price for those days. In order to prevent this being a problem, the missing values are replaced with the last available price. While this may not reflect the change in price resulting from non-market orders placed over the weekend, it should solve the problem of the missing values without altering the key elements of the data. Normally, a 
```{r}
ImputeMissing <- function(ts){
  df <- data.frame(ts)
#  Missing <- integer(nrow(df))
#  df = cbind(df, Missing)
  for(i in 1:nrow(df)){
    if(is.na(df[i, 1])){
      df[i, 1] <- df[i-1, 1]
#      df[i, 2] <- 1
    }
  }
  dates_formatted <- data.frame(dates)
  df <- cbind(df, dates_formatted)
  names(df)[1] <- "Price"
  names(df)[2] <- "Date"
  tib_new <- as_tsibble(df, index="Date") 
#                  start = c(2014, 32),
#                  frequency=365)
  
  return(tib_new)
}
```

```{r}
hsbc_tib <- ImputeMissing(hsbc_ts)
hsbc_ts <- as.ts(hsbc_tib, start = c(2014, as.numeric(format(dates[1], "%j"))), frequency=365)
```

```{r}
head(hsbc_tib)
```

```{r}
hsbc_visual <- hsbc_tib %>% 
  ggplot(aes(Date, Price)) +
  geom_line() +
  theme_minimal() +
  labs(title = "HSBC Stock Price", x = "Year", y = "Price")

print(hsbc_visual)
```

```{r}
#acf(hsbc_tib, lag.max = 10, na.action=na.pass)
```

```{r}
adf.test(hsbc_tib$Price)
```
The ADF test revealed that the timeseries is  unambiguously non-stationary. In order to make the data easier for the model to process, we will difference the data and repeat the ADF test. Assuming the timeseries is I(1), a first-difference series should be stationary.


```{r}
hsbc_log_diff <- hsbc_ts %>%
  log() %>%
  diff(1)
```

```{r}
head(hsbc_log_diff)
```

```{r}
plot(hsbc_log_diff)
```

```{r}
adf.test(hsbc_log_diff)
```
We can see from the results, that this first-difference logged series is indeed stationary. 

```{r}
hsbc_lagged <- embed(hsbc_log_diff, 21)
```

```{r}
split_num_val <- round(nrow(hsbc_lagged)*0.7)
split_num_train <- round(nrow(hsbc_lagged)*0.6)
hsbc_train <- data.frame(hsbc_lagged[1: split_num_train, ])
hsbc_val <- data.frame(hsbc_lagged[(split_num_train+1):split_num_val, ])
hsbc_test <- data.frame(hsbc_lagged[(split_num_val+1):nrow(hsbc_lagged), ])
X_train <- data.matrix(hsbc_train[, -1])
X_train_lstm <- array(X_train, c(dim(X_train), 1))
y_train <- hsbc_train[, 1]
X_val <- data.matrix(hsbc_val[, -1])
X_val_lstm <- array(X_val, c(dim(X_val), 1))
y_val <- hsbc_val[, 1]
X_test <- data.matrix(hsbc_test[, -1])
X_test_lstm <- array(X_test, c(dim(X_test), 1))
y_test <- hsbc_test[, 1] %>%
  cumsum() %>%
  exp()
y_test <- y_test * as.vector(tail(hsbc_ts, 1))
val_list <- list(X_val_lstm, y_val)
```

```{r}
head(hsbc_train)
```

## Models and Analysis
Now that we have the data, we can begin to attempt 
```{r}
linear <- lm(X1 ~ X2 + X3 + X4 + X5 + X6 + X7 + X8 + X9 + X10 + X11 + X12 + X13 + X14 + X15 + X16 + X17 + X18 + X19 + X20 + X21, data=hsbc_train)
summary(linear)
```

```{r}
linear_predicted <- predict.lm(linear, newdata = data.frame(hsbc_test)) %>% 
  cumsum() %>%
  exp()
linear_predicted <- linear_predicted * as.vector(tail(hsbc_ts, 1))
results <- data.frame(cbind(y_test, linear_predicted))
names(results)[1] <- "actual"
#mse_linear <- mean((results$actual - results$linear_predicted)**2)
#mse_linear
linear_accuracy <- accuracy(linear_predicted, y_test)

```
As expected, the standard linear regression has a very low explanatory and indeed predictive power. The lags that do seem to appear significant are likely just a result of the repeated "0" values every weekend (we took the first difference of the data and prices do not change over the weekend as there is no trading). Nonetheless we have a baseline model with results in favour of market efficiency: past prices do not contain any significant explanatory power (at least linearly).

In this problem of univariate time series prediction, we don't have a large body of covariates (or even any, discounting the lagged values) in the dataset. As a result, penalisation methods such as lasso and/or elastic nets may not be the most appropriate models for this problem. If there does exist a model where the lagged values of the dependent variable (alone) are able to provide decent explanatory power, then the relationships are likely to be highly non-linear. As a result, random forests may be a good model for capturing these relationships.

```{r}
rf <- randomForest(X_train, y_train, xtest=X_val, ytest=y_val, keep.forest = TRUE)
rf
```

```{r}
rf_predicted <- predict(rf, X_test) %>%
  cumsum() %>%
  exp()
rf_predicted <- rf_predicted * as.vector(tail(hsbc_ts, 1))
results <- data.frame(cbind(results, rf_predicted))
head(results)
```

```{r}
#mse_rf <- mean((results$actual - results$rf_predicted)**2)
#mse_rf
rf_accuracy <- accuracy(rf_predicted, y_test)
```
The random forests actually seem to provide a worse outcome than the linear model! There may be a few reasons for this. Firstly, it may simply be that there is no casual relationship between the dependent variable and its lags. Alternatively, it may be that the random forest model may not be the best for capturing it. The binary tree mechanism used within random forests are such that the model is unable to extrapolate, meaning that any values that are outside of the training set, cannot be predicted. THe random nature of selecting which features (in this case, lags) to use for each tree also means that any underlying latent trends in the priceseries cannot be captured.

```{r}
xgb <- xgboost(data=X_train, label=y_train, nrounds=120)
xgb
```

```{r}
xgb_predicted <- predict(xgb, X_test) %>%
  cumsum() %>%
  exp()
xgb_predicted <-xgb_predicted * as.vector(tail(hsbc_ts, 1))
```

```{r}
head(xgb_predicted)
```

```{r}
results <- data.frame(cbind(results, xgb_predicted))
head(results)
xgb_accuracy <- accuracy(xgb_predicted, y_test)
```

```{r}
svm <- svm()
```

```{r}
dim(X_train_lstm)[-1]
```

```{r}
X_train_lstm <- array(X_train, c(dim(X_train), 1))
```

```{r}
glimpse(y_train)
```

```{r}
lstm <- keras_model_sequential() %>%
  layer_lstm(units=32, 
             activation="tanh", #Default actiavtion for LSTM
             return_sequences = FALSE, #Only one LSTM layer hence sequence of outputs not needed
             input_shape = dim(X_train_lstm)[-1]) %>% #The layer takes as an input 20 timesteps (lags) and 1 feature
  layer_dropout(0.2) %>%
  layer_dense(units=1) #Linear activation used rather than RELu/SELu as output can be positive or negative
```

```{r}
summary(lstm)
```

```{r}
lstm %>% compile(optimizer="nadam", loss="mean_squared_error")
```

```{r}
lstm_model <- lstm %>% fit(x=X_train_lstm,
                           y=y_train,
                           batch_size=32,
                           epochs=20,
                           validation_data=val_list,
                           shuffle=FALSE)
```

```{r}
lstm_predicted <- predict(lstm, X_test_lstm) %>%
  cumsum() %>%
  exp()
lstm_predicted <-lstm_predicted * as.vector(tail(hsbc_ts, 1))
```

```{r}
head(lstm_model)
```

```{r}
results <- data.frame(cbind(results, lstm_predicted))
head(results)
lstm_accuracy <- accuracy(lstm_predicted, y_test)
```

## Conclusion

